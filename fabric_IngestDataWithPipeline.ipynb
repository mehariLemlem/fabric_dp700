{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0d6543",
   "metadata": {},
   "source": [
    "By combining the **pipeline** and **Spark** capabilities in **Fabric**, you can implement complex data ingestion logic that copies data from external sources into the **OneLake** storage on which the lakehouse is based, and then uses Spark code to perform custom data transformations before loading it into tables for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde9ab4",
   "metadata": {},
   "source": [
    "## Create Workspace ##\n",
    "1) Navigate to the [Microsoft Fabric home page](https://app.fabric.microsoft.com/home?experience=fabric)\n",
    "2) In the menu bar on the left, select Workspaces (the icon looks similar to ðŸ—‡)\n",
    "3) Create a new workspace with a name of your choice, selecting a licensing mode that includes Fabric capacity (Trial, Premium, or Fabric)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffacda",
   "metadata": {},
   "source": [
    "## Create Lakehouse ##\n",
    "1) Inside the workspace, click **+ New**.\n",
    "2) Select Lakehouse under the **Data Engineering** section.\n",
    "3) Click **Create**.\n",
    "\n",
    "Once created, you Lakehouse will have:\n",
    "- Tables folder â†’ structured Delta tables\n",
    "- Files folder â†’ unstructured / raw data\n",
    "- Built-in SQL endpoint for querying\n",
    "- Auto-created Power BI semantic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bc2ba",
   "metadata": {},
   "source": [
    "## Create a pipeline (Copy Data activity) ##\n",
    "1) **Home** page (lakehouse) â†’ Get data â†’ New data pipeline\n",
    "2) Copy Data â†’ Use copy assistant â†’ Choose data source (Copy Data wizard) â†’ Choose HTTP (New Resource)\n",
    "    - URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv\n",
    "    - Connection: Create new connection\n",
    "    - Connection name: Specify a unique name\n",
    "    - Data gateway: (none)\n",
    "    - Authentication kind: Anonymous\n",
    "3) Connect Data Source â†’ (Enter appropriate data) â†’ Next\n",
    "4) Connected to Data Destination â†’ (Enter appropriate data) â†’ Next â†’ Save + Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b255b",
   "metadata": {},
   "source": [
    "## Create a notebook ##\n",
    "1) Home â†’ Open notebook â†’ New notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155cf820",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Toggle parameter cell\n",
    "table_name = \"sales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee82cfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read the new sales data\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n",
    "\n",
    "## Add month and year columns\n",
    "df = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n",
    "\n",
    "# Derive FirstName and LastName columns\n",
    "df = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
    "\n",
    "# Filter and reorder columns\n",
    "df = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n",
    "# Load the data into a table\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
